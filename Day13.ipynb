{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9Lkm95ikwwx6",
        "eD1a2eKAw3Da",
        "-0dCeESZw7gI",
        "_iU0S9ZnxEE0",
        "h-VTpdQxxJnp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fortune-max/M4-python-refresher-ml/blob/main/Day13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rZgSNV76PslA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression:"
      ],
      "metadata": {
        "id": "VSYkC8ycutP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Predict student scores based on the number of hours they studied.\n",
        "\n",
        "Explanation: Use linear regression to model the relationship between hours studied and exam scores.\\\n",
        "Dataset: Student Performance Data - https://archive.ics.uci.edu/dataset/320/student+performance\n"
      ],
      "metadata": {
        "id": "lBY8tH4ewIOn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6ucdlEMxOQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Estimate house prices based on features like square footage, number of bedrooms, and location.\n",
        "\n",
        "Explanation: Apply linear regression with multiple features to predict house prices.\\\n",
        "Dataset: California Housing Prices\n",
        "https://www.kaggle.com/datasets/camnugent/california-housing-prices"
      ],
      "metadata": {
        "id": "lUMcFlCqwKPd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "108LKi6dxO6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict the temperature based on historical weather data.\n",
        "\n",
        "Explanation: Utilize linear regression to predict temperature using historical weather features.\\\n",
        "Dataset: Weather History\\\n",
        "https://www.kaggle.com/datasets/selfishgene/historical-hourly-weather-data"
      ],
      "metadata": {
        "id": "80Delp1kwMh0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DS2_vDCoxP3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analyze the relationship between a person's age and their income.\n",
        "\n",
        "Explanation: Use linear regression to examine the correlation between age and income.\\\n",
        "Dataset: Adult Income\n",
        "https://archive.ics.uci.edu/dataset/2/adult\n"
      ],
      "metadata": {
        "id": "AH2a8RfywOAK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HY8sZq4pxQfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression:\n"
      ],
      "metadata": {
        "id": "NvVeR1aYwebn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a spam email classifier using a dataset of labeled emails.\n",
        "\n",
        "Explanation: Employ logistic regression for binary classification of spam and non-spam emails.\\\n",
        "Dataset: Spambase\n",
        "https://archive.ics.uci.edu/dataset/94/spambase\n"
      ],
      "metadata": {
        "id": "chkcjidNwgDN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "texrkU8_xR2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOtSS7HExT9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict whether a customer will churn based on their usage patterns.\n",
        "\n",
        "Explanation: Utilize logistic regression to model customer churn prediction.\\\n",
        "Dataset: Telco Customer Churn\n",
        "https://www.kaggle.com/datasets/blastchar/telco-customer-churn"
      ],
      "metadata": {
        "id": "mJim3xmzwhsg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBrH4zGVxU41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Determine whether a bank loan will be approved based on customer information.\n",
        "\n",
        "Explanation: Use logistic regression for binary classification of loan approval.\\\n",
        "Dataset: Credit Approval\n",
        "https://archive.ics.uci.edu/dataset/27/credit+approval\n"
      ],
      "metadata": {
        "id": "huAY_DEAwj35"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkenWOkLxVO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classify images of handwritten digits using the MNIST dataset.\n",
        "\n",
        "Explanation: Apply logistic regression for multiclass classification of handwritten digits.\\\n",
        "Dataset: MNIST\n",
        "https://yann.lecun.com/exdb/mnist/"
      ],
      "metadata": {
        "id": "_A3mDorbwl2x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1h8qudvMxWBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest:\n"
      ],
      "metadata": {
        "id": "urSsBS2jwnQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a model to predict the likelihood of a customer buying a product based on various features.\n",
        "\n",
        "Explanation: Use a random forest classifier for predicting customer purchase likelihood.\\\n",
        "Dataset: Online Shopper's Intention\n",
        "https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset\n"
      ],
      "metadata": {
        "id": "eRQue451woLf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMMXHbLNxXNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict the type of flower based on characteristics like petal length and width.\n",
        "\n",
        "Explanation: Apply a random forest for multiclass classification of iris flowers.\\\n",
        "Dataset: Iris Dataset\\\n",
        "https://archive.ics.uci.edu/dataset/53/iris"
      ],
      "metadata": {
        "id": "usDpJAmZwpna"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAMENQz6xXtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a credit scoring model using multiple decision trees.\n",
        "\n",
        "Explanation: Utilize a random forest for credit scoring based on customer features.\n",
        "Dataset: Give Me Some Credit\n"
      ],
      "metadata": {
        "id": "n4fOz4SAwq4g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auwQWFbQxYEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use a random forest to detect fraudulent transactions in a financial dataset.\n",
        "\n",
        "Explanation: Implement a random forest for binary classification of fraudulent transactions.\n",
        "Dataset: Credit Card Fraud Detection\n"
      ],
      "metadata": {
        "id": "emn5UCyWwr9y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RGh3tEaexYXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LASSO and Ridge Regression:\n",
        "Predict housing prices using LASSO regression with feature regularization.\n",
        "\n",
        "Explanation: Apply LASSO regression to handle feature selection in predicting house prices.\n",
        "Dataset: California Housing Prices\n"
      ],
      "metadata": {
        "id": "NcCAAylhwtHi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1GLjCtaxY0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Estimate stock prices based on historical market data using Ridge regression.\n",
        "\n",
        "Explanation: Use Ridge regression to model stock prices with regularization.\n",
        "Dataset: Historical Stock Prices\n"
      ],
      "metadata": {
        "id": "6Et2FdntwuXc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9ES5StnxZHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analyze the impact of different regularization parameters on model performance.\n",
        "\n",
        "Explanation: Experiment with various regularization strengths and observe their effect on model performance.\n"
      ],
      "metadata": {
        "id": "EwI4QaEZwvoz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhtTq097xZZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jysRGOFMZjn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT (Natural Language Processing):\n"
      ],
      "metadata": {
        "id": "9Lkm95ikwwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8VAQwSWxZyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perform sentiment analysis on movie reviews using BERT embeddings.\n",
        "\n",
        "Explanation: Use BERT embeddings and a classifier for sentiment analysis.\n",
        "Dataset: IMDb Movie Reviews\n"
      ],
      "metadata": {
        "id": "2JZ2eWYgwxsK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZOqieZWxbqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a text classification model to identify topics in news articles.\n",
        "\n",
        "Explanation: Utilize BERT for topic classification in news articles.\n",
        "Dataset: AG News Classification\n"
      ],
      "metadata": {
        "id": "VQbGIK9Iwy8j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNl8QgbQxccC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a question-answering system using BERT for a given set of documents.\n",
        "\n",
        "Explanation: Implement BERT for question-answering on a dataset with questions and corresponding documents.\n",
        "Dataset: SQuAD - Stanford Question Answering Dataset\n"
      ],
      "metadata": {
        "id": "WJ-REMaow0UM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYY3jqPExc0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement a named entity recognition model using BERT for text data.\n",
        "\n",
        "Explanation: Use BERT for recognizing named entities in text.\n",
        "Dataset: NER (Named Entity Recognition) Kaggle Competition\n"
      ],
      "metadata": {
        "id": "brFcoZHLw1xc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAU5DrL_xdFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mixed Algorithms:\n"
      ],
      "metadata": {
        "id": "eD1a2eKAw3Da"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtpCuxL7xdVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combine linear regression and random forest for predicting stock prices.\n",
        "\n",
        "Explanation: Create an ensemble model using predictions from both linear regression and random forest.\n"
      ],
      "metadata": {
        "id": "mN49lPIuw4A4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkAHEPRlxdvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use logistic regression and BERT embeddings for classifying online customer reviews.\n",
        "\n",
        "Explanation: Combine logistic regression and BERT embeddings for sentiment analysis on customer reviews.\n"
      ],
      "metadata": {
        "id": "bZqFiOeHw5LF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8dsa8Zbxd-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Employ LASSO regression with random forest features for predicting customer churn.\n",
        "\n",
        "Explanation: Combine LASSO regression for feature selection with random forest for customer churn prediction.\n"
      ],
      "metadata": {
        "id": "08qQ-Dphw6Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fumqt4S4xeMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced Topics:\n"
      ],
      "metadata": {
        "id": "-0dCeESZw7gI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TFsPlTzxeaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement transfer learning with BERT for a custom NLP task.\n",
        "\n",
        "Explanation: Fine-tune a pre-trained BERT model on a specific NLP task.\n"
      ],
      "metadata": {
        "id": "e6XsoEJhw8ho"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrXtVNTlxen9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explore hyperparameter tuning for random forest using grid search or random search.\n",
        "\n",
        "Explanation: Optimize hyperparameters of a random forest model using grid search or random search.\n"
      ],
      "metadata": {
        "id": "1rBGZstHw_Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1jCgyS8xe2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build an ensemble model combining predictions from linear regression, logistic regression, and random forest.\n",
        "\n",
        "Explanation: Combine predictions from different models to create an ensemble model.\n"
      ],
      "metadata": {
        "id": "aJVFdh86xA5h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AabiA2SqxfEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation and Comparison:\n"
      ],
      "metadata": {
        "id": "_iU0S9ZnxEE0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tU-tb_yMxfRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate the performance of different algorithms on a common dataset.\n",
        "\n",
        "Explanation: Compare metrics such as accuracy, precision, recall, and F1 score across multiple algorithms.\n"
      ],
      "metadata": {
        "id": "I0eoKTMZxFX5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4_WKG2KxfkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare the interpretability of linear regression and random forest models.\n",
        "\n",
        "Explanation: Assess and discuss the interpretability of results from linear regression and random forest models.\n"
      ],
      "metadata": {
        "id": "lcCLF6U0xG0O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-Vrgc2fxgKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Investigate the impact of feature scaling on the performance of logistic regression.\n",
        "\n",
        "Explanation: Experiment with and analyze the effect of feature scaling on logistic regression performance.\n"
      ],
      "metadata": {
        "id": "bTOAYX5wxIKY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJGsauAlxgcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Real-World Applications:\n"
      ],
      "metadata": {
        "id": "h-VTpdQxxJnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply machine learning to healthcare by predicting disease outcomes based on patient data.\n",
        "\n",
        "Explanation: Use machine learning to predict disease outcomes using a healthcare dataset.\n"
      ],
      "metadata": {
        "id": "jQZnaE4GxLGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWTA7andxiD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solve a business problem, such as customer churn prediction, using a combination of algorithms and feature engineering.\n",
        "\n",
        "Explanation: Apply machine learning techniques to address a real-world business problem, considering both algorithmic choice and feature engineering.\n"
      ],
      "metadata": {
        "id": "T5blE5TYxMxi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyZKIxrYxiT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answers\n"
      ],
      "metadata": {
        "id": "rkfZvPhsxkH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('student_performance.csv')\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data[['hours_studied']]\n",
        "y = data['exam_scores']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "mhoimxFJxlmD",
        "outputId": "8783521a-bfdf-4b06-8792-7bc1b9fa64dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e49ee01964ae>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'student_performance.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Split the data into features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'student_performance.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('spam_email_dataset.csv')  # Replace 'spam_email_dataset.csv' with your dataset file\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data['text']\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a bag-of-words representation of the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "classification_rep = classification_report(y_test, predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{classification_rep}')\n"
      ],
      "metadata": {
        "id": "Q2QnuyEhy4Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 16"
      ],
      "metadata": {
        "id": "j2_BzjxM0Wbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running the code, make sure to install the required libraries:\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install sklearn\n"
      ],
      "metadata": {
        "id": "tat6kbbtze3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('movie_reviews_dataset.csv')  # Replace 'movie_reviews_dataset.csv' with your dataset file\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data['text']\n",
        "y = data['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Tokenize and encode the training data\n",
        "X_train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "y_train_tensor = torch.tensor(y_train.tolist())\n",
        "\n",
        "# Tokenize and encode the testing data\n",
        "X_test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "y_test_tensor = torch.tensor(y_test.tolist())\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):  # You can adjust the number of epochs\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(**X_train_encodings, labels=y_train_tensor)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**X_test_encodings)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Predictions\n",
        "predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "# Convert tensor predictions to list\n",
        "predictions_list = predictions.cpu().numpy().tolist()\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions_list)\n",
        "classification_rep = classification_report(y_test, predictions_list)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{classification_rep}')"
      ],
      "metadata": {
        "id": "wi7sm9epzkBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 18"
      ],
      "metadata": {
        "id": "B5lTWDiq0MAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n",
        "pip install torch\n"
      ],
      "metadata": {
        "id": "Wc_8jpa80NMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model for question-answering\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "def answer_question(context, question):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "\n",
        "    # Get the logits for the start and end positions\n",
        "    start_logits, end_logits = model(**inputs).logits.split(1, dim=-1)\n",
        "\n",
        "    # Find the tokens with the highest probability for start and end positions\n",
        "    start_index = torch.argmax(start_logits)\n",
        "    end_index = torch.argmax(end_logits)\n",
        "\n",
        "    # Get the answer span\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())\n",
        "    answer = tokenizer.decode(inputs['input_ids'][0][start_index:end_index+1].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "context = \"Bert is a natural language processing model.\"\n",
        "question = \"What is Bert?\"\n",
        "\n",
        "answer = answer_question(context, question)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "tXBjaX0L0PmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 27"
      ],
      "metadata": {
        "id": "Y2FiM_WU052r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For linear regression, the interpretability is relatively high. The coefficients associated with each feature provide information on the strength and direction of the relationship between the features and the target variable. Positive coefficients indicate a positive correlation, and negative coefficients indicate a negative correlation.\n",
        "\n",
        "On the other hand, random forests, being an ensemble of decision trees, are generally less interpretable. The combination of multiple decision trees makes it challenging to intuitively understand the relationship between features and predictions. However, there are some ways to gain insights:"
      ],
      "metadata": {
        "id": "_wHtq-ln0-Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with your dataset file\n",
        "\n",
        "# Assuming 'target' is the target variable\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Random forest\n",
        "rf_model = RandomForestRegressor()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Compare interpretability\n",
        "linear_coefs = pd.DataFrame({'Feature': X.columns, 'Coefficient': linear_model.coef_})\n",
        "print('Linear Regression Coefficients:')\n",
        "print(linear_coefs)\n",
        "\n",
        "# Feature importance for random forest\n",
        "rf_feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf_model.feature_importances_})\n",
        "print('\\nRandom Forest Feature Importance:')\n",
        "print(rf_feature_importance)\n",
        "\n",
        "# Compare predicted values\n",
        "linear_predictions = linear_model.predict(X_test)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Compare performance (e.g., mean squared error)\n",
        "linear_mse = mean_squared_error(y_test, linear_predictions)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "print(f'\\nLinear Regression Mean Squared Error: {linear_mse}')\n",
        "print(f'Random Forest Mean Squared Error: {rf_mse}')\n",
        "\n",
        "# Visualize feature importance for random forest\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(rf_feature_importance['Feature'], rf_feature_importance['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "so9zO0Fm1AGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, linear regression coefficients are directly accessible through the coef_ attribute, providing a clear indication of feature importance. For the random forest model, the feature_importances_ attribute is used to extract feature importance scores, which can be visualized for comparison.\n",
        "\n",
        "Keep in mind that the actual interpretability depends on the specific characteristics of your dataset and modeling goals. The choice between linear regression and random forest also involves trade-offs in terms of model complexity and predictive performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3kiurY-p1C_6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uT9hBhJ1G3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}